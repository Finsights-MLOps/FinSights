{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802fccaa",
   "metadata": {},
   "source": [
    "## STAGE 3: S3 VECTORS (NEW)\n",
    "```\n",
    "├─ key (int64 surrogate)\n",
    "├─ embedding (1024-d float32 vector)\n",
    "├─ metadata_filterable (20 bytes: cik_int, report_year, section_id, ...)\n",
    "├─ metadata_non_filterable (30 bytes: embedding_id, context_hint)\n",
    "├─ Purpose: Optimized for semantic search with AWS-managed ANN\n",
    "```\n",
    "\n",
    "- Stage 1 fact is authoritative, consolidated fact, pure data engineering result.\n",
    "- Stage 2 meta table REMAINS authoritative for both INFO + Embeddings layer (ML). Stage 2 - Lean table + Meta Fact table.\n",
    "- Stage 3 S3 Table is a search index projection.\n",
    "\n",
    "\n",
    "\n",
    "### Execution flow:\n",
    "1. cache_stage2_meta_table()              ` # Ensure Stage 2 available `\n",
    "2. cache_embeddings_table()               ` # Ensure embeddings available `\n",
    "3. build_s3vectors_stage3()               ` # Transform + join → Stage 3 `\n",
    "4. initialize_s3vectors_table()           ` # Upload to S3 (if INIT=True) `\n",
    "5. cache_s3vectors_table()                ` # Download/cache locally `\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "1. build_s3vectors_stage3(meta_df, vectors_df, provider)\n",
    "   └─> Core transformation logic\n",
    "   \n",
    "2. cache_s3vectors_table(config, provider, force_recache)\n",
    "   └─> Download from S3, cache locally\n",
    "   \n",
    "3. initialize_s3vectors_table(config, provider, df_stage3, force_reinit)\n",
    "   └─> Upload to S3, handle overwrites\n",
    "   \n",
    "4. validate_s3vectors_schema(df, expected_dims)\n",
    "   └─> Schema validation helper\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc265f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f099ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "======================================================================\n",
      "CONFIG VALIDATION - S3 VECTORS PATHS\n",
      "======================================================================\n",
      "\n",
      "✓ Base path: ML_EMBED_ASSETS/S3_VECTORS_STAGING\n",
      "\n",
      "✓ Provider: cohere_1024d\n",
      "  S3 Path: ML_EMBED_ASSETS/S3_VECTORS_STAGING/cohere_1024d/finrag_embeddings_s3vectors_cohere_1024d.parquet\n",
      "  Dimensions: 1024d\n",
      "\n",
      "✓ Provider: titan_1024d\n",
      "  S3 Path: ML_EMBED_ASSETS/S3_VECTORS_STAGING/titan_1024d/finrag_embeddings_s3vectors_titan_1024d.parquet\n",
      "  Dimensions: 1024d\n",
      "\n",
      "======================================================================\n",
      "AUTO-DETECTION TEST (provider=None)\n",
      "======================================================================\n",
      "  Default model: cohere_embed_v4\n",
      "  Auto-detected path: ML_EMBED_ASSETS/S3_VECTORS_STAGING/cohere_1024d/finrag_embeddings_s3vectors_cohere_1024d.parquet\n",
      "  Auto-detected dims: 1024d\n",
      "\n",
      "======================================================================\n",
      "LOCAL CACHE PATH VALIDATION\n",
      "======================================================================\n",
      "\n",
      "✓ Provider: cohere_1024d\n",
      "  Cache: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\stage3_s3vectors\\cohere_1024d\\finrag_embeddings_s3vectors_cohere_1024d.parquet\n",
      "  Exists: False\n",
      "\n",
      "✓ Provider: titan_1024d\n",
      "  Cache: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\stage3_s3vectors\\titan_1024d\\finrag_embeddings_s3vectors_titan_1024d.parquet\n",
      "  Exists: False\n",
      "\n",
      "======================================================================\n",
      "✓ Phase 1 Config Validation Complete\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# PATH VALIDATION: Validate S3 vectors paths from configuration\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent / 'loaders'))\n",
    "\n",
    "from ml_config_loader import MLConfig\n",
    "\n",
    "config = MLConfig()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIG VALIDATION - S3 VECTORS PATHS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test 1: Base path\n",
    "print(f\"\\n✓ Base path: {config.s3vectors_base_path}\")\n",
    "\n",
    "# Test 2: Provider-specific paths\n",
    "for provider in config.s3vectors_providers:\n",
    "    s3_path = config.s3vectors_path(provider)\n",
    "    dims = config.s3vectors_dimensions(provider)\n",
    "    print(f\"\\n✓ Provider: {provider}\")\n",
    "    print(f\"  S3 Path: {s3_path}\")\n",
    "    print(f\"  Dimensions: {dims}d\")\n",
    "\n",
    "# Test 3: Auto-detection (None provider)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"AUTO-DETECTION TEST (provider=None)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Default model: {config.bedrock_default_model_key}\")\n",
    "print(f\"  Auto-detected path: {config.s3vectors_path(None)}\")\n",
    "print(f\"  Auto-detected dims: {config.s3vectors_dimensions(None)}d\")\n",
    "\n",
    "# Test 4: Local cache paths\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"LOCAL CACHE PATH VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for provider in config.s3vectors_providers:\n",
    "    cache_path = config.get_s3vectors_cache_path(provider)\n",
    "    print(f\"\\n✓ Provider: {provider}\")\n",
    "    print(f\"  Cache: {cache_path}\")\n",
    "    print(f\"  Exists: {cache_path.exists()}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"✓ Phase 1 Config Validation Complete\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0922150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ mmh3 installed successfully\n",
      "  Test hash: -7804031895798801076\n",
      "  Type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import mmh3\n",
    "\n",
    "# Test basic functionality\n",
    "test_hash = mmh3.hash64(\"test_sentence_id\", signed=True)[0]\n",
    "print(f\"✓ mmh3 installed successfully\")\n",
    "print(f\"  Test hash: {test_hash}\")\n",
    "print(f\"  Type: {type(test_hash)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbedcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f81f8a0b",
   "metadata": {},
   "source": [
    "### Decisions for S3 Vector table:\n",
    "\n",
    "```\n",
    "# Stage 2 → Stage 3 mapping (CONFIRMED):\n",
    "✓ sentenceID           → sentenceID\n",
    "✓ cik_int              → cik_int\n",
    "✓ report_year          → report_year (exists, not 'year')\n",
    "✓ section_name         → section_name (exists)\n",
    "✓ sic                  → sic\n",
    "✓ section_sentence_count → section_sentence_count\n",
    "\n",
    "# Embeddings table:\n",
    "✓ embedding_id         → embedding_id\n",
    "✓ embedding            → embedding\n",
    "\n",
    "# Derived:\n",
    "→ sentenceID_numsurrogate (mmh3 hash)\n",
    "→ sentence_pos (extract from sentenceID with fallback)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d16e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "======================================================================\n",
      "S3 VECTORS PIPELINE (Stage 3)\n",
      "======================================================================\n",
      "Provider: cohere_1024d\n",
      "Model: cohere.embed-v4:0 (1024d)\n",
      "\n",
      "[Loading Dependencies]\n",
      "  ✓ Stage 2 Meta: 469,252 rows\n",
      "  ✓ Embeddings: 203,076 rows\n",
      "\n",
      "[Stage 3 Build - cohere_1024d]\n",
      "  Input Meta: 469,252 rows × 34 cols\n",
      "  Input Vectors: 203,076 rows × 3 cols\n",
      "  After join: 203,076 rows\n",
      "  Computing mmh3 hashes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joems\\AppData\\Local\\Temp\\ipykernel_25880\\1293568731.py:140: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  hash_counts = df_joined.group_by('sentenceID_numsurrogate').agg(pl.count().alias('n'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Hash uniqueness validated (0 collisions)\n",
      "\n",
      "  ✓ Stage 3 Complete:\n",
      "    Rows: 203,076\n",
      "    Columns: 10\n",
      "    Dimensions: 1024d (validated)\n",
      "\n",
      "  ✓ Cached locally: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\stage3_s3vectors\\cohere_1024d\\finrag_embeddings_s3vectors_cohere_1024d.parquet\n",
      "\n",
      "[S3 Vectors Table - Recreating]\n",
      "  Provider: cohere_1024d\n",
      "  ✓ Deleted existing\n",
      "\n",
      "[S3 Vectors Table - Creating]\n",
      "  Provider: cohere_1024d\n",
      "  Destination: s3://sentence-data-ingestion/ML_EMBED_ASSETS/S3_VECTORS_STAGING/cohere_1024d/finrag_embeddings_s3vectors_cohere_1024d.parquet\n",
      "  Rows: 203,076\n",
      "  Dimensions: 1024d\n",
      "  ✓ Upload complete (Cost: $0.00 ingress)\n",
      "\n",
      "======================================================================\n",
      "✓ S3 VECTORS PIPELINE COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Actions completed:\n",
      "  ✓ Stage 3 built locally (203,076 rows)\n",
      "  ✓ Uploaded to S3\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# S3 VECTORS PIPELINE (Stage 3) - Execution Parameters\n",
    "\n",
    "# - Stage 3 is a JOIN operation (cheap, ~1-2 min)\n",
    "# - Depends on Stage 1 + Stage 2 (upstream changes)\n",
    "# - No complex merge logic needed (just rebuild)\n",
    "# ============================================================================\n",
    "\n",
    "# Build Stage 3 locally\n",
    "BUILD_S3VECTORS_TABLE = True      # Create Stage 3 from Stage 2 + Embeddings\n",
    "\n",
    "# S3 Table Initialization\n",
    "UPLOAD_TO_S3  = True      # Upload Stage 3 to S3\n",
    "FORCE_OVERWRITE_S3  = True    # Overwrite existing S3 table\n",
    "\n",
    "# Local Caching -- Not needed with always rebuild concept. DELETED.\n",
    "# CACHE_S3VECTORS_LOCALLY = False   # Download from S3 and cache\n",
    "# FORCE_RECACHE_S3VECTORS = False   # Re-download even if cached\n",
    "\n",
    "# Provider Selection\n",
    "S3VECTORS_PROVIDER = \"cohere_1024d\"  # 'cohere_1024d' or 'titan_1024d'\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import polars as pl\n",
    "import mmh3\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent / 'loaders'))\n",
    "\n",
    "from ml_config_loader import MLConfig\n",
    "import polars as pl\n",
    "import mmh3\n",
    "\n",
    "# Helper function (from DATA PREP)\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def check_s3_exists(s3_client, bucket, s3_key):\n",
    "    \"\"\"Check if S3 object exists\"\"\"\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=s3_key)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            return False\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def _egress_cost_usd(bytes_count: int) -> float:\n",
    "    \"\"\"Estimate S3 egress cost\"\"\"\n",
    "    gb = bytes_count / (1024 * 1024 * 1024)\n",
    "    return gb * 0.09\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Standard: ends with numeric sequence → extract it\n",
    "# 2. Malformed: no numeric end → default to -1 (flag for investigation)\n",
    "\n",
    "def extract_sentence_position(sentenceID_series):\n",
    "    \"\"\"\n",
    "    Extract sentence position from sentenceID with fallback\n",
    "    \n",
    "    Logic:\n",
    "    - Split by '_', take last segment\n",
    "    - If numeric → cast to int16\n",
    "    - If non-numeric → return -1 (sentinel value)\n",
    "    \n",
    "    Sentinel value choice: -1 vs 99 vs NULL\n",
    "    - -1: Clear indicator of extraction failure, sorts first\n",
    "    - 99: Ambiguous (could be real position)\n",
    "    - NULL: Causes filtering issues in S3 Vectors\n",
    "    \n",
    "    Recommendation: Use -1\n",
    "    \"\"\"\n",
    "    return (\n",
    "        pl.col(sentenceID_series)\n",
    "        .str.split('_')\n",
    "        .list.last()\n",
    "        .cast(pl.Int16, strict=False)  # strict=False → NULL on cast failure\n",
    "        .fill_null(-1)                  # NULL → -1 sentinel\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def build_s3vectors_stage3(meta_df, vectors_df, provider, config):\n",
    "    \"\"\"\n",
    "    Transform Stage 2 (meta) + Embeddings → Stage 3 (S3 Vectors ready)\n",
    "    \n",
    "    Args:\n",
    "        meta_df: Stage 2 meta table (34 cols, 1M rows)\n",
    "        vectors_df: Embeddings table (3 cols, ~900K rows)\n",
    "        provider: 'cohere_1024d' or 'titan_1024d'\n",
    "        config: MLConfig instance\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 10 columns, ready for S3 Vectors ingestion\n",
    "    \n",
    "    Schema:\n",
    "        sentenceID_numsurrogate (int64)\n",
    "        sentenceID (varchar)\n",
    "        embedding (f32[1024])\n",
    "        cik_int (int32)\n",
    "        report_year (int16)\n",
    "        section_name (varchar)\n",
    "        sic (varchar)\n",
    "        sentence_pos (int16)\n",
    "        embedding_id (varchar)\n",
    "        section_sentence_count (int16)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n[Stage 3 Build - {provider}]\")\n",
    "    print(f\"  Input Meta: {len(meta_df):,} rows × {len(meta_df.columns)} cols\")\n",
    "    print(f\"  Input Vectors: {len(vectors_df):,} rows × {len(vectors_df.columns)} cols\")\n",
    "    \n",
    "    # STEP 1: Inner Join (only embedded sentences)\n",
    "    df_joined = (\n",
    "        meta_df\n",
    "        .join(vectors_df, on='sentenceID', how='inner')\n",
    "        .filter(pl.col('embedding_id').is_not_null())  # Safety filter\n",
    "    )\n",
    "    \n",
    "    print(f\"  After join: {len(df_joined):,} rows\")\n",
    "    \n",
    "    if len(df_joined) == 0:\n",
    "        raise ValueError(f\"No embeddings found for provider {provider}. Check join keys.\")\n",
    "    \n",
    "    # STEP 2: Derive sentenceID_numsurrogate (mmh3 hash)\n",
    "    print(f\"  Computing mmh3 hashes...\")\n",
    "    \n",
    "    # Convert sentenceID to hash using mmh3\n",
    "    sentence_ids = df_joined['sentenceID'].to_list()\n",
    "    hashes = [mmh3.hash64(sid, signed=True)[0] for sid in sentence_ids]\n",
    "    \n",
    "    df_joined = df_joined.with_columns([\n",
    "        pl.Series('sentenceID_numsurrogate', hashes, dtype=pl.Int64)\n",
    "    ])\n",
    "    \n",
    "    # Validate hash uniqueness\n",
    "    hash_counts = df_joined.group_by('sentenceID_numsurrogate').agg(pl.count().alias('n'))\n",
    "    collisions = hash_counts.filter(pl.col('n') > 1)\n",
    "    \n",
    "    if len(collisions) > 0:\n",
    "        print(f\"  ⚠️  WARNING: {len(collisions)} hash collisions detected\")\n",
    "        print(f\"     Collision rate: {len(collisions)/len(df_joined)*100:.4f}%\")\n",
    "        # Note: At 1M scale, collisions are extremely rare with mmh3\n",
    "    else:\n",
    "        print(f\"  ✓ Hash uniqueness validated (0 collisions)\")\n",
    "    \n",
    "    # STEP 3: Extract sentence_pos with fallback\n",
    "    df_joined = df_joined.with_columns([\n",
    "        pl.col('sentenceID')\n",
    "          .str.split('_')\n",
    "          .list.last()\n",
    "          .cast(pl.Int16, strict=False)\n",
    "          .fill_null(-1)\n",
    "          .alias('sentence_pos')\n",
    "    ])\n",
    "    \n",
    "    # Diagnostic: Check how many failed extraction\n",
    "    failed_extractions = df_joined.filter(pl.col('sentence_pos') == -1).height\n",
    "    if failed_extractions > 0:\n",
    "        print(f\"  ⚠️  {failed_extractions} sentences with position extraction failure (set to -1)\")\n",
    "    \n",
    "    # STEP 4: Select & Order Final Columns\n",
    "    df_stage3 = df_joined.select([\n",
    "        # Primary keys\n",
    "        'sentenceID_numsurrogate',\n",
    "        'sentenceID',\n",
    "        \n",
    "        # Embedding\n",
    "        'embedding',\n",
    "        \n",
    "        # Filterable metadata\n",
    "        'cik_int',\n",
    "        'report_year',\n",
    "        'section_name',\n",
    "        'sic',\n",
    "        'sentence_pos',\n",
    "        \n",
    "        # Non-filterable metadata\n",
    "        'embedding_id',\n",
    "        'section_sentence_count'\n",
    "    ])\n",
    "    \n",
    "    # STEP 5: Validate Schema\n",
    "    expected_dims = config.s3vectors_dimensions(provider)\n",
    "    actual_dims = df_stage3['embedding'].list.len()[0]\n",
    "    \n",
    "    if actual_dims != expected_dims:\n",
    "        raise ValueError(\n",
    "            f\"Dimension mismatch for {provider}:\\n\"\n",
    "            f\"  Expected: {expected_dims}d (from config)\\n\"\n",
    "            f\"  Actual: {actual_dims}d (in embedding column)\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n  ✓ Stage 3 Complete:\")\n",
    "    print(f\"    Rows: {len(df_stage3):,}\")\n",
    "    print(f\"    Columns: {len(df_stage3.columns)}\")\n",
    "    print(f\"    Dimensions: {actual_dims}d (validated)\")\n",
    "    \n",
    "    return df_stage3\n",
    "\n",
    "\n",
    "\n",
    "def initialize_s3vectors_table(config, provider, df_stage3, force_reinit=False):\n",
    "    \"\"\"\n",
    "    Upload Stage 3 table to S3 (provider-specific)\n",
    "    \n",
    "    Args:\n",
    "        config: MLConfig instance\n",
    "        provider: 'cohere_1024d' or 'titan_1024d'\n",
    "        df_stage3: Transformed DataFrame\n",
    "        force_reinit: If True, overwrite existing S3 table\n",
    "    \"\"\"\n",
    "    \n",
    "    s3_client = config.get_s3_client()\n",
    "    s3_key = config.s3vectors_path(provider)\n",
    "    s3_uri = f\"s3://{config.bucket}/{s3_key}\"\n",
    "    \n",
    "    # Check if exists\n",
    "    exists = check_s3_exists(s3_client, config.bucket, s3_key)\n",
    "    \n",
    "    if exists and not force_reinit:\n",
    "        print(f\"\\n[S3 Vectors Table - Already Exists]\")\n",
    "        print(f\"  Provider: {provider}\")\n",
    "        print(f\"  Location: {s3_uri}\")\n",
    "        print(f\"  Set FORCE_OVERWRITE_S3 =True to recreate\")\n",
    "        return\n",
    "    \n",
    "    elif exists and force_reinit:\n",
    "        print(f\"\\n[S3 Vectors Table - Recreating]\")\n",
    "        print(f\"  Provider: {provider}\")\n",
    "        s3_client.delete_object(Bucket=config.bucket, Key=s3_key)\n",
    "        print(f\"  ✓ Deleted existing\")\n",
    "    \n",
    "    # Upload to S3\n",
    "    print(f\"\\n[S3 Vectors Table - Creating]\")\n",
    "    print(f\"  Provider: {provider}\")\n",
    "    print(f\"  Destination: {s3_uri}\")\n",
    "    print(f\"  Rows: {len(df_stage3):,}\")\n",
    "    print(f\"  Dimensions: {config.s3vectors_dimensions(provider)}d\")\n",
    "    \n",
    "    df_stage3.write_parquet(\n",
    "        s3_uri,\n",
    "        storage_options=config.get_storage_options(),\n",
    "        compression='zstd'\n",
    "    )\n",
    "    \n",
    "    print(f\"  ✓ Upload complete (Cost: $0.00 ingress)\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# wont be called for now.\n",
    "def cache_s3vectors_table(config, provider, force_recache=False):\n",
    "    \"\"\"\n",
    "    Download and cache Stage 3 table locally\n",
    "    \n",
    "    Args:\n",
    "        config: MLConfig instance\n",
    "        provider: 'cohere_1024d' or 'titan_1024d'\n",
    "        force_recache: If True, re-download even if cached\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame loaded from cache or S3\n",
    "    \"\"\"\n",
    "    \n",
    "    cache_path = config.get_s3vectors_cache_path(provider)\n",
    "    \n",
    "    # Check local cache first\n",
    "    if not force_recache and cache_path.exists():\n",
    "        print(f\"\\n[S3 Vectors Table - Using Cache]\")\n",
    "        print(f\"  Provider: {provider}\")\n",
    "        print(f\"  Location: {cache_path.name}\")\n",
    "        df = pl.read_parquet(cache_path)\n",
    "        print(f\"  Loaded: {len(df):,} rows × {len(df.columns)} columns (Cost: $0.00)\")\n",
    "        return df\n",
    "    \n",
    "    # Download from S3\n",
    "    print(f\"\\n[S3 Vectors Table - Downloading from S3]\")\n",
    "    print(f\"  Provider: {provider}\")\n",
    "    \n",
    "    s3_key = config.s3vectors_path(provider)\n",
    "    s3_uri = f\"s3://{config.bucket}/{s3_key}\"\n",
    "    \n",
    "    s3_client = config.get_s3_client()\n",
    "    \n",
    "    # Check if exists on S3\n",
    "    if not check_s3_exists(s3_client, config.bucket, s3_key):\n",
    "        raise FileNotFoundError(\n",
    "            f\"S3 Vectors table not found for {provider}!\\n\"\n",
    "            f\"  Expected: {s3_uri}\\n\"\n",
    "            f\"  Run with UPLOAD_TO_S3 =True to create\"\n",
    "        )\n",
    "    \n",
    "    response = s3_client.head_object(Bucket=config.bucket, Key=s3_key)\n",
    "    file_size_mb = response['ContentLength'] / 1024 / 1024\n",
    "    egress_cost = _egress_cost_usd(response['ContentLength'])\n",
    "    \n",
    "    print(f\"  Source: {s3_uri}\")\n",
    "    print(f\"  Size: {file_size_mb:.1f} MB\")\n",
    "    \n",
    "    df = pl.read_parquet(s3_uri, storage_options=config.get_storage_options())\n",
    "    print(f\"  Downloaded: {len(df):,} rows (Cost: ${egress_cost:.4f} egress)\")\n",
    "    \n",
    "    # Cache for future use\n",
    "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.write_parquet(cache_path, compression='zstd')\n",
    "    print(f\"  ✓ Cached to: {cache_path}\")\n",
    "    \n",
    "    return df\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "config = MLConfig()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"S3 VECTORS PIPELINE (Stage 3)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Provider: {S3VECTORS_PROVIDER}\")\n",
    "print(f\"Model: {config.bedrock_model_id} ({config.s3vectors_dimensions(S3VECTORS_PROVIDER)}d)\")\n",
    "\n",
    "df_stage3 = None\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1: Build Stage 3 Table Locally\n",
    "# ============================================================================\n",
    "\n",
    "if BUILD_S3VECTORS_TABLE:\n",
    "    # Load dependencies\n",
    "    print(f\"\\n[Loading Dependencies]\")\n",
    "    \n",
    "    # Stage 2 meta table\n",
    "    meta_cache = Path.cwd().parent / 'data_cache' / 'meta_embeds' / 'finrag_fact_sentences_meta_embeds.parquet'\n",
    "    if not meta_cache.exists():\n",
    "        raise FileNotFoundError(f\"Stage 2 meta not cached: {meta_cache}\")\n",
    "    meta_df = pl.read_parquet(meta_cache)\n",
    "    print(f\"  ✓ Stage 2 Meta: {len(meta_df):,} rows\")\n",
    "    \n",
    "    # Embeddings table\n",
    "    emb_filename = Path(config.embeddings_path(S3VECTORS_PROVIDER)).name\n",
    "    emb_cache = Path.cwd().parent / 'data_cache' / 'embeddings' / S3VECTORS_PROVIDER / emb_filename\n",
    "    if not emb_cache.exists():\n",
    "        raise FileNotFoundError(f\"Embeddings not cached: {emb_cache}\")\n",
    "    vectors_df = pl.read_parquet(emb_cache)\n",
    "    print(f\"  ✓ Embeddings: {len(vectors_df):,} rows\")\n",
    "    \n",
    "    # Build Stage 3\n",
    "    df_stage3 = build_s3vectors_stage3(meta_df, vectors_df, S3VECTORS_PROVIDER, config)\n",
    "    \n",
    "    # Cache locally\n",
    "    cache_path = config.get_s3vectors_cache_path(S3VECTORS_PROVIDER)\n",
    "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_stage3.write_parquet(cache_path, compression='zstd')\n",
    "    print(f\"\\n  ✓ Cached locally: {cache_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 2: Initialize S3 Table\n",
    "# ============================================================================\n",
    "\n",
    "if UPLOAD_TO_S3 :\n",
    "    if df_stage3 is None:\n",
    "        # Load from local cache\n",
    "        cache_path = config.get_s3vectors_cache_path(S3VECTORS_PROVIDER)\n",
    "        if not cache_path.exists():\n",
    "            raise FileNotFoundError(f\"Stage 3 not built. Run with BUILD_S3VECTORS_TABLE=True first.\")\n",
    "        df_stage3 = pl.read_parquet(cache_path)\n",
    "    \n",
    "    initialize_s3vectors_table(config, S3VECTORS_PROVIDER, df_stage3, force_reinit=FORCE_OVERWRITE_S3 )\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 3: Cache from S3\n",
    "# ============================================================================\n",
    "\n",
    "#  DELETED. DELETED. DELETED. DELETED.\n",
    "# if CACHE_S3VECTORS_LOCALLY:\n",
    "#     df_cached = cache_s3vectors_table(config, S3VECTORS_PROVIDER, force_recache=FORCE_RECACHE_S3VECTORS)\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ S3 VECTORS PIPELINE COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nActions completed:\")\n",
    "if BUILD_S3VECTORS_TABLE:\n",
    "    print(f\"  ✓ Stage 3 built locally ({len(df_stage3):,} rows)\")\n",
    "if UPLOAD_TO_S3 :\n",
    "    print(f\"  ✓ Uploaded to S3\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b4830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 203,076\n",
      "Columns: ['sentenceID_numsurrogate', 'sentenceID', 'embedding', 'cik_int', 'report_year', 'section_name', 'sic', 'sentence_pos', 'embedding_id', 'section_sentence_count']\n",
      "\n",
      "Sample:\n",
      "shape: (3, 10)\n",
      "┌────────────┬────────────┬────────────┬─────────┬───┬──────┬────────────┬────────────┬────────────┐\n",
      "│ sentenceID ┆ sentenceID ┆ embedding  ┆ cik_int ┆ … ┆ sic  ┆ sentence_p ┆ embedding_ ┆ section_se │\n",
      "│ _numsurrog ┆ ---        ┆ ---        ┆ ---     ┆   ┆ ---  ┆ os         ┆ id         ┆ ntence_cou │\n",
      "│ ate        ┆ str        ┆ list[f32]  ┆ i32     ┆   ┆ str  ┆ ---        ┆ ---        ┆ nt         │\n",
      "│ ---        ┆            ┆            ┆         ┆   ┆      ┆ i16        ┆ str        ┆ ---        │\n",
      "│ i64        ┆            ┆            ┆         ┆   ┆      ┆            ┆            ┆ u32        │\n",
      "╞════════════╪════════════╪════════════╪═════════╪═══╪══════╪════════════╪════════════╪════════════╡\n",
      "│ -610014736 ┆ 0001403161 ┆ [0.025757, ┆ 1403161 ┆ … ┆ 7389 ┆ 90         ┆ bedrock_co ┆ 292        │\n",
      "│ 6912333961 ┆ _10-K_2020 ┆ 0.001572,  ┆         ┆   ┆      ┆            ┆ here_v4_10 ┆            │\n",
      "│            ┆ _section_1 ┆ … -0.0319… ┆         ┆   ┆      ┆            ┆ 24d_202511 ┆            │\n",
      "│            ┆ …          ┆            ┆         ┆   ┆      ┆            ┆ …          ┆            │\n",
      "│ 4157098702 ┆ 0000059478 ┆ [-0.011658 ┆ 59478   ┆ … ┆ 2834 ┆ 413        ┆ bedrock_co ┆ 768        │\n",
      "│ 459027656  ┆ _10-K_2018 ┆ ,          ┆         ┆   ┆      ┆            ┆ here_v4_10 ┆            │\n",
      "│            ┆ _section_8 ┆ -0.020142, ┆         ┆   ┆      ┆            ┆ 24d_202511 ┆            │\n",
      "│            ┆ …          ┆ … -0.00…   ┆         ┆   ┆      ┆            ┆ …          ┆            │\n",
      "│ 8302680116 ┆ 0001276520 ┆ [0.008362, ┆ 1276520 ┆ … ┆ 6311 ┆ 32         ┆ bedrock_co ┆ 691        │\n",
      "│ 527838924  ┆ _10-K_2019 ┆ 0.024536,  ┆         ┆   ┆      ┆            ┆ here_v4_10 ┆            │\n",
      "│            ┆ _section_1 ┆ … 0.00604… ┆         ┆   ┆      ┆            ┆ 24d_202511 ┆            │\n",
      "│            ┆ …          ┆            ┆         ┆   ┆      ┆            ┆ …          ┆            │\n",
      "└────────────┴────────────┴────────────┴─────────┴───┴──────┴────────────┴────────────┴────────────┘\n",
      "\n",
      "Sentence position extraction failures: 0\n"
     ]
    }
   ],
   "source": [
    "## Quick Validation Cells 1 - 4. \n",
    "\n",
    "cache_path = config.get_s3vectors_cache_path(\"cohere_1024d\")\n",
    "df_check = pl.read_parquet(cache_path)\n",
    "\n",
    "print(f\"Rows: {len(df_check):,}\")\n",
    "print(f\"Columns: {df_check.columns}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(df_check.head(3))\n",
    "\n",
    "# Check for -1 sentence_pos (extraction failures)\n",
    "failed = df_check.filter(pl.col('sentence_pos') == -1).height\n",
    "print(f\"\\nSentence position extraction failures: {failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c37f88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joems\\AppData\\Local\\Temp\\ipykernel_25880\\1753585404.py:1: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  df_check.group_by('cik_int').agg(pl.count().alias('n')).sort('n', descending=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (21, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cik_int</th><th>n</th></tr><tr><td>i32</td><td>u32</td></tr></thead><tbody><tr><td>1276520</td><td>20442</td></tr><tr><td>1273813</td><td>17425</td></tr><tr><td>813762</td><td>17326</td></tr><tr><td>890926</td><td>15033</td></tr><tr><td>814585</td><td>12532</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>200406</td><td>5405</td></tr><tr><td>909832</td><td>5030</td></tr><tr><td>1018724</td><td>4903</td></tr><tr><td>1065280</td><td>4861</td></tr><tr><td>104169</td><td>3254</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (21, 2)\n",
       "┌─────────┬───────┐\n",
       "│ cik_int ┆ n     │\n",
       "│ ---     ┆ ---   │\n",
       "│ i32     ┆ u32   │\n",
       "╞═════════╪═══════╡\n",
       "│ 1276520 ┆ 20442 │\n",
       "│ 1273813 ┆ 17425 │\n",
       "│ 813762  ┆ 17326 │\n",
       "│ 890926  ┆ 15033 │\n",
       "│ 814585  ┆ 12532 │\n",
       "│ …       ┆ …     │\n",
       "│ 200406  ┆ 5405  │\n",
       "│ 909832  ┆ 5030  │\n",
       "│ 1018724 ┆ 4903  │\n",
       "│ 1065280 ┆ 4861  │\n",
       "│ 104169  ┆ 3254  │\n",
       "└─────────┴───────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.group_by('cik_int').agg(pl.count().alias('n')).sort('n', descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6daa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joems\\AppData\\Local\\Temp\\ipykernel_25880\\3204195042.py:1: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  df_check.group_by('report_year').agg(pl.count().alias('n')).sort('report_year')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>report_year</th><th>n</th></tr><tr><td>i64</td><td>u32</td></tr></thead><tbody><tr><td>2015</td><td>11471</td></tr><tr><td>2016</td><td>39615</td></tr><tr><td>2017</td><td>35033</td></tr><tr><td>2018</td><td>37853</td></tr><tr><td>2019</td><td>37034</td></tr><tr><td>2020</td><td>42070</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 2)\n",
       "┌─────────────┬───────┐\n",
       "│ report_year ┆ n     │\n",
       "│ ---         ┆ ---   │\n",
       "│ i64         ┆ u32   │\n",
       "╞═════════════╪═══════╡\n",
       "│ 2015        ┆ 11471 │\n",
       "│ 2016        ┆ 39615 │\n",
       "│ 2017        ┆ 35033 │\n",
       "│ 2018        ┆ 37853 │\n",
       "│ 2019        ┆ 37034 │\n",
       "│ 2020        ┆ 42070 │\n",
       "└─────────────┴───────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.group_by('report_year').agg(pl.count().alias('n')).sort('report_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8409e476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1024\n",
      "Embedding type: <class 'polars.series.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Check embedding dimensions\n",
    "sample_embedding = df_check['embedding'][0]\n",
    "print(f\"Embedding length: {len(sample_embedding)}\")\n",
    "print(f\"Embedding type: {type(sample_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669d24aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 203,076\n",
      "Unique hashes: 203,076\n",
      "Collisions: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify no hash collisions\n",
    "unique_hashes = df_check['sentenceID_numsurrogate'].n_unique()\n",
    "total_rows = len(df_check)\n",
    "\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(f\"Unique hashes: {unique_hashes:,}\")\n",
    "print(f\"Collisions: {total_rows - unique_hashes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7035f",
   "metadata": {},
   "source": [
    "## Post Code success.\n",
    "\n",
    "- 'sentenceID', 'embedding_id', 'section_sentence_count'. \n",
    "- ['sentenceID_numsurrogate',  'embedding', 'cik_int', 'report_year', 'section_name', 'sic', 'sentence_pos' ]\n",
    "  \n",
    "- sentenceID_numsurrogate → Vector ID (primary key)\n",
    "- embedding → Vector values (1024d array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd392a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292789c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinRAG ML (Python 3.11)",
   "language": "python",
   "name": "finrag_ml_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
