## Frontend for FinSights Model Pipeline

This document outlines the design for the serving layer of the FinSights Model Pipeline, focusing on the frontend and backend architecture using Streamlit, Pydantic and FastAPI.

### Clean separation:
- T1: FastAPI backend with uvicorn (persistent event loop)
- T2: Streamlit frontend (separate process, makes HTTP requests)
- This is the clean separation architecture - not direct Python imports in Streamlit.

```
SCRIPT (to):
Start â†’ Run â†’ Finish â†’ Exit

FRONTEND - BACKEND (concept):
Start â†’ Initialize â†’ Listen... â†’ Handle Request â†’ Listen... â†’ Handle Request â†’ Listen... [Forever]
                                    â†‘                              â†‘
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         Event Loop ( Magic :D )
```


### event_loop / uvicorn event loop:
```
Server "alive" (listening)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROCESS: uvicorn (PID 12345)            â”‚
â”‚ STATE: Running (infinite loop)          â”‚
â”‚ PORT: 8000 (bound to TCP socket)        â”‚
â”‚ MEMORY: Contains all ML dependencies    â”‚
â”‚         (boto3, polars, code)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘
     â”‚ Waiting for HTTP requests...
     â”‚ (blocks here until request arrives)
```


### Overview:
```
ðŸ“¦serving
 â”£ ðŸ“‚.streamlit
 â”ƒ â”— ðŸ“œconfig.toml
 â”£ ðŸ“‚backend
 â”ƒ â”£ ðŸ“œapi_service.py
 â”ƒ â”£ ðŸ“œconfig.py
 â”ƒ â”£ ðŸ“œmodels.py
 â”ƒ â”£ ðŸ“œrequirements.txt
 â”ƒ â”— ðŸ“œ__init__.py
 â”£ ðŸ“‚frontend
 â”ƒ â”£ ðŸ“œapi_client.py
 â”ƒ â”£ ðŸ“œchat.py
 â”ƒ â”£ ðŸ“œmetrics.py
 â”ƒ â”£ ðŸ“œrequirements.txt
 â”ƒ â”£ ðŸ“œstate.py
 â”ƒ â”— ðŸ“œ__init__.py
 â”£ ðŸ“œ.env.example.txt
 â”£ ðŸ“œFRONTEND_DESIGN.md
 â”£ ðŸ“œrun_dev.sh
 â”— ðŸ“œServing_SETUP.md
```

Edit `.env` to customize:
- `BACKEND_PORT`: Backend API port (default: 8000)
- `FRONTEND_PORT`: Frontend UI port (default: 8501)
- `LOG_LEVEL`: Logging verbosity
- `ENABLE_CACHE`: Query result caching




### Potential Process Flow:
```
1. USER types in Streamlit: "What was Apple's revenue?"
                â†“
2. STREAMLIT makes HTTP call:
   requests.post("http://localhost:8000/query", 
                 json={"question": "What was Apple's revenue?"})
                â†“
3. FASTAPI receives at @app.post("/query"):
   - Validates input with Pydantic, Extracts question from request
                â†“
4. CONTROLLER calls backend:
   result = answer_query(query=request.question, ...)  # â† CODE
                â†“
5. ORCHESTRATOR runs ():
                â†“
6. CONTROLLER packages response:
   return QueryResponse(answer=result['answer'], ...)
                â†“
7. STREAMLIT receives and displays:
   st.write(response.json()['answer'])
```

1. Use Native-component container Queries. Not in-line python or css forced injections; unmaintainable.
2. Session State / Streamlit has built-in theming since v1.10.
3. Streamlit's native components just handle 99%
4. Native theming via TOML, Container components for layout. 



#### Pattern - clean ideas:
```
# app.py - Clean routing
if st.session_state.page == "Home":
    render_home()
elif st.session_state.page == "Chatbot":
    render_chatbot()
```



### Step-by-Step: User Asks "What was Apple's revenue?"**
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. USER TYPES IN BROWSER                                    â”‚
â”‚    - Browser shows Streamlit UI at localhost:8501           â”‚
â”‚    - User types: "What was Apple's 2023 revenue?"           â”‚
â”‚    - Clicks submit                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. BROWSER â†’ STREAMLIT (HTTP)                               â”‚
â”‚    POST http://localhost:8501/_stcore/stream                â”‚
â”‚    Body: {user input, session state, etc.}                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. STREAMLIT PROCESS (PID 12346)                            â”‚
â”‚    - Receives HTTP from browser                             â”‚
â”‚    - Executes Python code:                                  â”‚
â”‚                                                             â”‚
â”‚    prompt = st.chat_input(...)  # Gets user text            â”‚
â”‚    result = requests.post(      # â† Makes HTTP call         â”‚
â”‚        "http://localhost:8000/query",                       â”‚
â”‚        json={"question": prompt}                            â”‚
â”‚    )                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ HTTP over loopback (localhost)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. UVICORN PROCESS (PID 12345)                              â”‚
â”‚    - Event loop wakes up (request arrived!)                 â”‚
â”‚    - Parses HTTP request                                    â”‚
â”‚    - Calls FastAPI route:                                   â”‚
â”‚                                                             â”‚
â”‚    @app.post("/query")                                      â”‚
â”‚    async def query_endpoint(request):                       â”‚
â”‚        result = answer_query(...)  # â† IN-MEMORY CALL       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ Function call (same process)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. ORCHESTRATOR (SAME PROCESS - PID 12345)                  â”‚  
â”‚    - answer_query() executes                                â”‚
â”‚    - Loads RAG components (already in memory)               â”‚
â”‚    - Calls AWS Bedrock (network request)                    â”‚
â”‚    - Processes response                                     â”‚
â”‚    - Returns Python dict                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ Return value
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. FASTAPI CONVERTS TO HTTP                                 â”‚
â”‚    - Takes Python dict                                      â”‚
â”‚    - Serializes to JSON                                     â”‚
â”‚    - Wraps in HTTP response                                 â”‚
â”‚    - Sends back to Streamlit                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ HTTP response
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. STREAMLIT RECEIVES RESPONSE                              â”‚
â”‚    - response.json() parses it                              â”‚
â”‚    - Updates UI with answer                                 â”‚
â”‚    - Sends new HTML to browser                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ HTTP response
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. BROWSER DISPLAYS RESULT                                  â”‚
â”‚    - User sees answer on screen                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

### Microservices potential architecture:
- may not do this but looking into this research. 
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  FastAPI            â”‚
â”‚  (venv_serving)  â”‚                 â”‚  (venv_serving)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚                     â”‚
                                     â”‚  Lightweight proxy  â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â”‚ HTTP/RPC
                                              â†“
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚  ML Service         â”‚
                                     â”‚  (venv_ml_rag)      â”‚
                                     â”‚                     â”‚
                                     â”‚  Runs orchestrator  â”‚
                                     â”‚  in separate processâ”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### If we plan on using GITHUB actions:
```
Code â†’ GitHub (storage) â†’ GitHub Actions (CI/CD) â†’ Streamlit Cloud/Railway/Alternative (hosting)
                                        â†“
                                 Run tests, checks
```
- Streamlit Cloud for frontend, Modal.com for serverless backend -- expensive. 
- A little out of scope now.
- Streamlit Cloud - AWS and data files not so possible? (investigating)

---

## Frontend Design:

```
Dependency Tree:
    api_client.py          # NO dependencies (pure HTTP client)
         â†“
    state.py               # Uses api_client for health checks ()
         â†“
    chat.py                # Needs BOTH api_client + state
         â†“
    metrics.py             # Needs state (to access metadata)
```

### Design Constraints
- NO CSS Hacks â†’ Use native Streamlit components only
- NO Custom JavaScript â†’ Pure Python/Streamlit
- Stateless Queries â†’ Each query independent, no conversation context
- Error Tolerance â†’ Graceful degradation if backend unavailable
- Performance â†’ Use st.cache_resource for backend client initialization


### Feature 1: Query Submission Flow
  1. User types question in st.chat_input()
  2. Validate input (min 10 chars)
  3. Display user message immediately
  4. Show loading spinner: "Processing query..."
  5. Call api_client.send_query()
  6. Handle response:
     - Success â†’ Display answer + metadata
     - Error â†’ Display error message with details
  7. Update session state with new message
  8. Scroll to bottom of chat

### Feature 2: Backend Health Check
   ON APP STARTUP:
   1. Call api_client.check_health()
   2. If healthy â†’ Show green indicator
   3. If unhealthy â†’ Show warning banner
   4. Store status in st.session_state.backend_healthy

### Feature 3: Metadata Display
   FOR EACH ASSISTANT MESSAGE:
   1. Show answer text prominently
   2. Add expandable section below answer:
      - LLM Info: Model, tokens, cost
      - Context Info: KPI/RAG flags, length
      - Processing Time
   3. Update cumulative cost tracker

### Feature 4: Error Handling
   ERROR TYPES TO HANDLE:
   - Connection Error â†’ "Backend not responding. Is it running?"
   - Timeout â†’ "Query took too long (>120s)"
   - Validation Error â†’ "Question too short (min 10 chars)"
   - Pipeline Error â†’ Display error.stage and error.error

### Feature 5: Chat History
   VISUAL ONLY (NO SEMANTIC MEMORY):
   - Display all past Q&A pairs on page load
   - Each query is independent (no context passed)
   - Optional: "Clear History" button in sidebar



#### RCA1 quick info:
- Root Cause: Two places are opening the browser:
- Streamlit's built-in auto-open (--server.headless false)
- backup script command (Start-Process "http://localhost:8501")
#### RCA2 and so on: No bugs, cosmetic warnings.
- SyntaxWarnings - Streamlit library has docstrings with \. that should be \\. or raw strings
- Config option 'server.enableCORS=false' is not compatible with 'server.enableXsrfProtection=true'. As a result, 'server.enableCORS' is being overridden to 'true'. Just informational.
- Python 3.12 made regex escape sequences stricter. Streamlit team needs to update their docstrings.
