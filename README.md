# FinSights

#### Course Project (MLOps IE7374) - FINRAG Insights.
- Building an AI-powered financial analysis pipeline for structured KPI extraction and explainable reporting from 10-K filings SEC(Securities and Exchange Commission).

## Project Overview:

1. For background, and Business HLD (High-Level Design) please feel free to skim through [Scoping](design_docs/Project_Scoping_IE7374_FinSights.pdf) and [Design](design_docs/Finance_RAG_HLD_v1.xlsx)(excel). They explain the business problem, solution approach, and high-level architecture.  
    - The Excel file contains dataset initial understanding, cloud cost estimates, tool research, and algorithm analysisâ€”essential reference for developers.

2. The DataPipeline module hosts the live SEC(Securities and Exchange Commission) data ingestion process. It's a step in **Data Preprocessing**, to handle crawl-download-parse and upload final structured filings to AWS S3 buckets. Main contents are the `DataPipeline/src` and it's related `DataPipeline/dag` which orchestrates it.

3. For initial data engineering, please refer to `DataPipeline/data_engineering_research` 
    - Here, [Data Engineering](DataPipeline/data_engineering_research/duckdb_data_engineering/Data_Engineering_README.md) and other README files document strategy, key technical achievements, data quality approach, sampling strategies, etc. `duckdb_data_engineering/sql` has DuckDB SQL scripts for number of operations. 
    - Files in `data_engineering_research/exploratory_research` has [Research](DataPipeline/data_engineering_research/exploratory_research/Research_README.md#L5) and massive sets of EDA, experiment scripts with polars, EDA-charts - [EDA Notes](DataPipeline/data_engineering_research/exploratory_research/polars_eda_research/Master_EDA_Notes.pdf) etc. 

4. `src_aws_etl/` has the code, tests, configs, and requirements for the AWS S3 based ETL pipeline (Merge, Archive, Logs). Main code files are in `src_aws_etl/etl/`. 
    - Here is where bulk historical data and live data merge meaningfully and cleanly. Archival of older data and log management is also handled here.


5. `src_metrics/` has the code, tests, configs, and requirements for the Data Ingestion pipeline, here we collect and process all the financial metrics(RAW numbers) from the 10-K SEC(Securities and Exchange Commission).

6. Following that, `data_auto_stats/` has a really good collection of modules for schema validation, data quality checks, automated testing and stat-generation using `great_expectations` and `anamoly detection and alerts`.


## Project Structure:
```
ğŸ“¦ FinSights-MLOps/
 â”£ ğŸ“‚ DataPipeline/                          # SEC data ingestion & ETL orchestration
 â”ƒ â”£ ğŸ“‚ dag/                                 # Airflow DAGs for workflow automation
 â”ƒ â”£ ğŸ“‚ src/                                 # SEC Edgar SDK ingestion + financial metrics extraction
 â”ƒ â”£ ğŸ“‚ src_aws_etl/                         # S3 merge strategies (incremental + historical), archival, logging
 â”ƒ â”£ ğŸ“‚ data_auto_stats/                     # Great Expectations validation, anomaly detection
 â”ƒ â”£ ğŸ“‚ data_engineering_research/           # DuckDB analytics, Polars EDA, SQL exploration
 â”ƒ â”£ ğŸ“œ docker-compose.yaml                  # Container orchestration
 â”ƒ â”— ğŸ“œ environment.yml                      # Conda environment spec
 â”ƒ
 â”£ ğŸ“‚ ModelPipeline/                         # LLM/RAG infrastructure & validation (finrag_ml_tg1/)
 â”ƒ â”£ ğŸ“‚ platform_core_notebooks/             # Embedding generation, S3 Vectors provisioning, Gold test curation
 â”ƒ â”ƒ â”£ ğŸ“œ 01_Stage2_EmbeddingGen.ipynb       # Stage 2 meta table + embedding pipeline
 â”ƒ â”ƒ â”£ ğŸ“œ 02_EmbeddingAnalytics.ipynb        # Vector-metadata parity, staleness audits
 â”ƒ â”ƒ â”£ ğŸ“œ 03_S3Vector_TableProvisioning.ipynb
 â”ƒ â”ƒ â”£ ğŸ“œ 04_S3Vector_BulkIngestion.ipynb
 â”ƒ â”ƒ â”£ ğŸ“œ 05_GoldP1P2_TestSuite.ipynb        # Anchor-based validation tests
 â”ƒ â”ƒ â”£ ğŸ“œ 06_GoldP3_HeuristicEng_Curation.ipynb
 â”ƒ â”ƒ â”— ğŸ“œ 07-09 (Cost, Architecture, Tests)
 â”ƒ â”ƒ
 â”ƒ â”£ ğŸ“‚ rag_modules_src/                     # Production RAG components (query-time execution)
 â”ƒ â”ƒ â”£ ğŸ“‚ entity_adapter/                    # Entity extraction, fuzzy matching, metric mapping
 â”ƒ â”ƒ â”£ ğŸ“‚ metric_pipeline/                   # Structured KPI extraction
 â”ƒ â”ƒ â”£ ğŸ“‚ rag_pipeline/                      # Retrieval, context assembly, provenance tracking
 â”ƒ â”ƒ â”£ ğŸ“‚ synthesis_pipeline/                # LLM response generation, citation validation
 â”ƒ â”ƒ â”£ ğŸ“‚ prompts/                           # YAML prompt templates
 â”ƒ â”ƒ â”— ğŸ“‚ utilities/                         # Logging, error handling, shared helpers
 â”ƒ â”ƒ
 â”ƒ â”£ ğŸ“‚ loaders/                             # MLConfig service, data loading utilities
 â”ƒ â”£ ğŸ“‚ data_cache/                          # Local Parquet mirrors, analysis exports
 â”ƒ â”£ ğŸ“‚ .aws_config/                         # AWS service configurations
 â”ƒ â”£ ğŸ“‚ .aws_secrets/                        # Credentials (gitignored)
 â”ƒ â”— ğŸ“œ ml_config.yaml                       # 200+ model/retrieval parameters
 â”ƒ
 â”£ ğŸ“‚ design_docs/                           # Architecture diagrams, flow charts
 â”ƒ
 â”£ ğŸ“œ README.md                              # Project overview & navigation
 â”£ ğŸ“œ ARCHITECTURE.md                        # Directory structure + pipeline flows
 â”£ ğŸ“œ IMPLEMENTATION_GUIDE.md                # Parts 1-10 technical deep-dive
 â”— ğŸ“œ LLMOPS_TECHNICAL_COMPLIANCE.md         # MLOps requirement mapping

```


## DVC : 
Data version Control has been implemented in this Repo, and the data is stored on an s3 Bucket managed by our team. The metadata is stored in the .dvc folder.
The DVC is to control the versions of the data used in the ingestion pipeline ,so if any data is lost / manipulated with , we can retreive the version needed.

## High level Conceptual Flow:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA ENGINEERING LAYER                                          â”‚
â”‚ SEC Edgar API â†’ Sentence Extraction â†’ S3 Storage (1M samples)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EMBEDDING & INDEXING LAYER                                      â”‚
â”‚ Cohere Embed v4 â†’ S3 Vectors (200K+ 1024-d) â†’ Metadata Filters â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG ORCHESTRATION LAYER                                         â”‚
â”‚ Entity Extraction â†’ Query Variants â†’ Triple Retrieval Paths    â”‚
â”‚ (Filtered + Global + Variants) â†’ Context Assembly              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SYNTHESIS & SERVING LAYER                                       â”‚
â”‚ Dual Supply Lines (KPI + Semantic) â†’ LLM (Claude Bedrock)      â”‚
â”‚ â†’ Citation Headers â†’ Structured Response                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- Data Pipeline Setup: https://github.com/Finsights-MLOps/FinSights/blob/main/DataPipeline/SETUP_README.md
- Data Pipeline Documentation: https://github.com/Finsights-MLOps/FinSights/blob/main/DataPipeline/README.md

### Source Dataset Links:
1. Primary: https://huggingface.co/datasets/khaihernlow/financial-reports-sec
2. Live Ingestion metrics: https://www.sec.gov/search-filings/edgar-application-programming-interfaces
3. SEC EDGAR API (company_tickers.json), State Street SPDR ETF holdings for S&P 500 constituents
2. Potentially used: EdgarTools https://github.com/dgunning/edgartools
4. Primary datasets' source citation: https://zenodo.org/records/5589195


