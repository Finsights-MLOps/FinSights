"""
Data loading utilities for SEC filings with S3 support
"""
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Optional, Dict, Any, Tuple
import logging
from datetime import datetime
import pyarrow.parquet as pq
import boto3
from io import BytesIO
import os
from dotenv import load_dotenv

from config import RAW_DATA_PATH, VALIDATION_CONFIG

logger = logging.getLogger(__name__)

class SECDataLoader:
    """Handle data loading for SEC filings from local or S3"""
    
    def __init__(self, data_path: Optional[str] = None, use_s3: bool = False, 
                 bucket_name: Optional[str] = None, s3_key: Optional[str] = None):
        """
        Initialize data loader
        
        Args:
            data_path: Local file path (used if use_s3=False)
            use_s3: Whether to load from S3
            bucket_name: S3 bucket name
            s3_key: S3 object key (path within bucket)
        """
        self.use_s3 = use_s3
        self.bucket_name = bucket_name
        self.s3_key = s3_key
        
        if use_s3:
            # Initialize S3 client
            self._init_s3()
            self.data_path = f"s3://{bucket_name}/{s3_key}"
        else:
            # Use local path
            if data_path:
                self.data_path = Path(data_path) if isinstance(data_path, str) else data_path
            else:
                raise ValueError("No data path provided. Either provide a local file path or configure S3.")
        
        self.df = None
        self.metadata = {}
    
    def _init_s3(self):
        """Initialize S3 client with credentials"""
        # Load AWS credentials from .aws_secrets/aws_credentials.env
        secrets_path = Path(__file__).parent.parent / '.aws_secrets' / 'aws_credentials.env'
        if secrets_path.exists():
            load_dotenv(secrets_path)
            logger.info(f"Loaded AWS credentials from {secrets_path}")
        else:
            logger.warning(f"AWS credentials file not found at {secrets_path}, using default credentials")
        
        # Initialize S3 client
        self.s3 = boto3.client(
            's3',
            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
            region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1')
        )
        
        # Test connection
        try:
            self.s3.head_bucket(Bucket=self.bucket_name)
            logger.info(f"Successfully connected to S3 bucket: {self.bucket_name}")
        except Exception as e:
            logger.error(f"Failed to connect to S3 bucket {self.bucket_name}: {e}")
            raise
    
    def load_data_from_s3(self) -> pd.DataFrame:
        """Load data directly from S3"""
        logger.info(f"Loading data from S3: s3://{self.bucket_name}/{self.s3_key}")
        
        try:
            # Get object from S3
            response = self.s3.get_object(Bucket=self.bucket_name, Key=self.s3_key)
            
            # Check file extension
            if self.s3_key.endswith('.parquet'):
                # Read parquet directly from bytes
                self.df = pd.read_parquet(BytesIO(response['Body'].read()))
                
            elif self.s3_key.endswith('.csv'):
                # Read CSV directly from bytes
                self.df = pd.read_csv(BytesIO(response['Body'].read()))
                
            else:
                raise ValueError(f"Unsupported file type for S3 key: {self.s3_key}")
            
            logger.info(f"Successfully loaded {len(self.df):,} records from S3")
            return self.df
            
        except Exception as e:
            logger.error(f"Error loading data from S3: {e}")
            raise
    
    def load_data(self, sample_size: Optional[int] = None) -> pd.DataFrame:
        """Load SEC filings data from local or S3"""
        
        if self.use_s3:
            # Load from S3
            self.df = self.load_data_from_s3()
        else:
            # Load from local file
            logger.info(f"Loading data from local file: {self.data_path}")
            
            try:
                # Ensure path is a Path object
                if not isinstance(self.data_path, Path):
                    self.data_path = Path(self.data_path)
                
                # Check if file exists
                if not self.data_path.exists():
                    raise FileNotFoundError(f"Data file not found: {self.data_path}")
                
                if self.data_path.suffix == '.parquet':
                    self.df = pd.read_parquet(self.data_path)
                elif self.data_path.suffix == '.csv':
                    self.df = pd.read_csv(self.data_path)
                else:
                    raise ValueError(f"Unsupported file type: {self.data_path.suffix}")
                    
            except Exception as e:
                logger.error(f"Error loading data: {e}")
                raise
        
        # Apply sampling if requested
        if sample_size and sample_size < len(self.df):
            self.df = self.df.sample(n=sample_size, random_state=42)
            logger.info(f"Sampled {sample_size} records from {len(self.df):,} total")
        
        self._collect_metadata()
        logger.info(f"Loaded {len(self.df):,} records with {len(self.df.columns)} columns")
        return self.df
    
    def _collect_metadata(self):
        """Collect metadata about loaded data"""
        self.metadata = {
            'file_path': str(self.data_path),
            'source': 'S3' if self.use_s3 else 'Local',
            'load_timestamp': datetime.now().isoformat(),
            'num_rows': len(self.df),
            'num_columns': len(self.df.columns),
            'columns': list(self.df.columns),
            'dtypes': {str(k): str(v) for k, v in self.df.dtypes.to_dict().items()},
            'memory_usage_mb': self.df.memory_usage(deep=True).sum() / 1024 / 1024
        }
        
        if self.use_s3:
            self.metadata['bucket'] = self.bucket_name
            self.metadata['s3_key'] = self.s3_key
    
    def validate_schema(self) -> Tuple[bool, Dict[str, Any]]:
        """Basic schema validation"""
        from config import SEC_SCHEMA_CONFIG
        
        expected = set(SEC_SCHEMA_CONFIG['expected_columns'])
        actual = set(self.df.columns)
        
        validation_result = {
            'is_valid': True,
            'missing_columns': list(expected - actual),
            'extra_columns': list(actual - expected),
            'column_count': len(actual)
        }
        
        if validation_result['missing_columns']:
            validation_result['is_valid'] = False
            logger.warning(f"Missing columns: {validation_result['missing_columns']}")
            
        return validation_result['is_valid'], validation_result
    
    def list_s3_files(self, prefix: Optional[str] = None) -> list:
        """List files in S3 bucket (utility method)"""
        if not self.use_s3:
            raise ValueError("S3 not configured")
        
        try:
            response = self.s3.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=prefix or ''
            )
            
            if 'Contents' in response:
                files = []
                for obj in response['Contents']:
                    files.append({
                        'key': obj['Key'],
                        'size_mb': obj['Size'] / (1024 * 1024),
                        'last_modified': obj['LastModified']
                    })
                return files
            return []
            
        except Exception as e:
            logger.error(f"Error listing S3 files: {e}")
            raise


def load_sec_data(path: Optional[str] = None, 
                  use_s3: bool = False,
                  bucket_name: Optional[str] = None,
                  s3_key: Optional[str] = None,
                  sample: bool = False) -> pd.DataFrame:
    """
    Convenience function to load SEC data from local or S3
    
    Args:
        path: Local file path (ignored if use_s3=True)
        use_s3: Whether to load from S3
        bucket_name: S3 bucket name
        s3_key: S3 object key
        sample: Whether to load sample data (will sample from loaded data)
    
    Returns:
        DataFrame with SEC filings data
        
    Raises:
        ValueError: If no data source is configured
        FileNotFoundError: If local file doesn't exist
    """
    
    if not use_s3 and not path:
        raise ValueError("Must provide either a file path or S3 configuration")
    
    loader = SECDataLoader(
        data_path=path,
        use_s3=use_s3,
        bucket_name=bucket_name,
        s3_key=s3_key
    )
    
    if sample:
        return loader.load_data(sample_size=1000)
    return loader.load_data()

if __name__ == "__main__":
    # Test loading from S3
    try:
        df = load_sec_data(
            use_s3=True,
            bucket_name='sentence-data-ingestion',
            s3_key='DATA_MERGE_ASSETS/FINRAG_FACT_SENTENCES/finrag_fact_sentences.parquet'
        )
        print(f"Loaded data: {df.shape}")
        print(f"Columns: {df.columns.tolist()}")
        print(df.head())
    except Exception as e:
        print(f"Failed to load data: {e}")